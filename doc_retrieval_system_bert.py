# -*- coding: utf-8 -*-
"""Doc_Retrieval_System_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cNPbzpIHpWfCjmQR3U9XCxjkcBWFK09L
"""

import tarfile

#Download from original source file cord-19_2020-03-13.tar.gz
"""
import requests

url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz'
r = requests.get(url, allow_redirects=True)
open('cord-19_2020-03-13.tar.gz', 'wb').write(r.content)

tf = tarfile.open('cord-19_2020-03-13.tar.gz')
tf.extractall()
"""
#Download from drive file comm_use_subset.tar.gz

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/file/d/id=1LKQXQ6sUSSQL3ymX0Arp8jk0v3CNNK4U'
fluff, id = link.split('=')
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('comm_use_subset.tar.gz')

tf = tarfile.open(name='comm_use_subset.tar.gz', mode='r:gz')
tf.extractall()

import re

def remove_sentence(sentences):
    if (sentences[-1] == '[CLS]') :
        return [sentences[-1]]
    return remove_sentence(sentences[:-1])

def parse_sentence(sentences) :
    temp = []
    while (sentences) :
        word = sentences.pop()
        if (word == '[CLS]') :
            return sentences, True
        if (word in key_words) :
            return sentences+[word]+temp, True
        temp = [word]+temp
    return [], False


def clean_text(text) :
    #Lowercase
    text = text.lower()
    
    # Seperate punctuation
    text = re.findall(r"[\w']+|[.,!?;]", text)

    new = []
    for i, word in enumerate(text) :
        if (word == '.' or word == '!') :
            # new.append(word)
            new.append('[SEP]')
        elif (word == '?') :
            new = remove_sentence(new)
        # elif word not in stop_words :
        else :
            new.append(word)
    # text = ' '.join(new)
    return new

unwanted_sections = {"Introduction", "Background", "Discussion", "Results",
                     "Results and discussion", "Method", "", "annex", "Annex", 
                     "General", "annexation", "Annexation", "introduction",
                     "background", "discussion", "results", "method", "general"}
                     
wanted_sections = {"conclusion", "conclusions", "Conclusion", "Conclusions",
                   "summary", "Summary", "Inference", "inference"
                   "Inferences", "inferences"}

import json
import os
import nltk
from nltk.tokenize import word_tokenize

def parse_document(document) :
    info_counter = 0
    info = {}

    info['indexed_tokens'] = {}
    info['sentences'] = {}
    info['segments_ids'] = {}
    for i in range(len(document['body_text'])) :
        if (document['body_text'][i]['section'] in unwanted_sections) :
            continue
        if (document['body_text'][i]['section'] not in wanted_sections) :
            continue
        marked_text = clean_text(document['body_text'][i]['text'])
        
        if (len(marked_text) <= 5) :
            continue
        sentence = []
        for j, word in enumerate(marked_text) :
            if (word == '[SEP]') :
                sentence.append(word)
                if (len(sentence) > 100) :
                    sentence = []
                    continue
                # tokenized_sentence = word_tokenazer(sentence)
                info['indexed_tokens'][info_counter] = tokenizer.convert_tokens_to_ids(sentence)
                info['segments_ids'][info_counter] = [1] * len(tokenized_sentence)
                sentence = ' '.join(sentence)
                info['sentences'][info_counter] = sentence
                info_counter += 1
                sentence = []
                continue
            sentence.append(word)
 
    return info

documents = {}
files = {}
doc_number = 0
# max_papers = 500
counter = 0
labels = []

for file in os.listdir('comm_use_subset'):
    # if (max_papers < counter) :
        # break
    counter += 1
    with open("comm_use_subset/"+file, "r") as document:
        document = json.load(document)
        files[doc_number] = document
        documents[doc_number] = parse_document(document)

    doc_number += 1

