# -*- coding: utf-8 -*-
"""Doc_Retrieval_System_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JOe5ni9K-VW5fH98bRfD_AdtuceUVbi
"""

import tarfile

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#Download from drive file comm_use_subset.tar.gz
link = 'https://drive.google.com/file/d/id=1LKQXQ6sUSSQL3ymX0Arp8jk0v3CNNK4U'
fluff, id = link.split('=')
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('comm_use_subset.tar.gz')

tf = tarfile.open(name='comm_use_subset.tar.gz', mode='r:gz')
tf.extractall()

import nltk
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import re

stop_words = set(stopwords.words('english'))

def remove_sentence(sentences):
    if (sentences[-1] == '[CLS]') :
        return [sentences[-1]]
    return remove_sentence(sentences[:-1])

def parse_sentence(sentences) :
    temp = []
    while (sentences) :
        word = sentences.pop()
        if (word == '[CLS]') :
            return sentences, True
        if (word in key_words) :
            return sentences+[word]+temp, True
        temp = [word]+temp
    return [], False


def clean_text(text) :
    #Lowercase
    text = text.lower()
    
    # Seperate punctuation
    text = re.findall(r"[\w']+|[.,!?;]", text)

    new = []
    for i, word in enumerate(text) :
        # word = lemmatizer.lemmatize(word)
        if i == 0 :
            new.append('[CLS]')
        if (word == '.' or word == '!') :
            # new.append(word)
            new.append('[SEP]')
            # new, flag = parse_sentence(new)
            if (i < len(text)-1) :
                new.append('[CLS]')
        elif (word == '?') :
            new = remove_sentence(new)
        # elif word not in stop_words :
        else :
            new.append(word)
    # text = ' '.join(new)
    return new

!pip install transformers

unwanted_sections = {"Introduction", "Background", "Method", "annex", "Annex", 
                     "General", "annexation", "Annexation", "introduction",
                     "background", "Background", "method", "general",
                     "INTRODUCTION", "BACKGROUND", "METHOD", "ANNEX", "GENERAL", 
                     "ANNEXATION"}

wanted_sections = {"conclusion", "conclusions", "Conclusion", "Conclusions",
                   "CONCLUSION", "CONCLUSIONS", "summary", "Summary", "SUMMARY" 
                   "Inference", "inference", "Inferences", "inferences", "INFERENCE",
                   "results", "Results", "RESULTS", "Discussion", "DISCUSSION",
                   "outcome", "OUTCOME", "Outcome"}

pip install bert-extractive-summarizer

from summarizer import Summarizer
from transformers import BertTokenizer

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
summer = Summarizer()

import json
import os

def parse_document(document) :
    info_counter = 0
    info = {}

    info['sentences'] = {}

    info['bert_indexed_tokens'] = {}
    info['bert_segments_ids'] = {}

    for i in range(len(document['body_text'])) :
        if (document['body_text'][i]['section'] in unwanted_sections) :
            continue
        # if (document['body_text'][i]['section'] not in wanted_sections) :
        #    continue
        summarized = summer(document['body_text'][i]['text'], ratio=0.3)
        # summarized_text = summarized[0]['summary_text']
        marked_text = clean_text(summarized)
        # marked_text = clean_text(document['body_text'][i]['text'])
        if (len(marked_text) <= 5) :
            continue
        sentence = []
        for j, word in enumerate(marked_text) :
            if (word == '[SEP]') :
                sentence.append(word)
                if (len(sentence) > 50) :
                    sentence = []
                    continue
                sentence = ' '.join(sentence)
                info['sentences'][info_counter] = sentence

                tokenized_sentence = bert_tokenizer.tokenize(sentence)
                info['bert_indexed_tokens'][info_counter] = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)
                info['bert_segments_ids'][info_counter] = [0] * len(tokenized_sentence)

                info_counter += 1
                sentence = []
                continue
            sentence.append(word)
    return info

documents = {}
files = {}
doc_number = 0
# max_papers = 500
counter = 0
labels = []

for file in os.listdir('comm_use_subset'):
    # if (max_papers < counter) :
    #   break
    counter += 1
    with open("comm_use_subset/"+file, "r") as document:
        document = json.load(document)
        files[doc_number] = document
        documents[doc_number] = parse_document(document)

    doc_number += 1

from transformers import BertModel 

# Load pre-trained model (weights)"

bert_model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True, # Whether the model returns all hidden-states.
                                  )

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

bert_model.to(device)

import torch

bert_doc_embeddings = {}
# documents[i][0] is info['indexed_tokens'])
# documents[i][1] is info['sentences'])
# documents[i][2] is info['segments_ids'])

for i in range(len(documents)) :
    bert_doc_embeddings[i] = {}
    for j in range(len(documents[i]['bert_indexed_tokens'])) :
        tokens_tensor = torch.tensor([documents[i]['bert_indexed_tokens'][j]]).to(device)
        segments_tensors = torch.tensor([documents[i]['bert_segments_ids'][j]]).to(device)
        with torch.no_grad() :
            outputs = bert_model(tokens_tensor, segments_tensors)
        hidden_states = outputs[2]
        token_vecs = hidden_states[-2][0]
        sentence_embedding = torch.mean(token_vecs, dim=0)
        bert_doc_embeddings[i][j] = sentence_embedding

questions = {
    0:"What are the coronoviruses?",
    1:"What was discovered in Wuhuan in December 2019?",
    2:"What is Coronovirus Disease 2019?",
    3:"What is COVID-19?",
    4:"What is caused by SARS-COV2?",
    5:"How is COVID-19 spread?",
    6:"Where was COVID-19 discovered?",
    7:"How does coronavirus spread?",
    8:"What is SARS-COV2?",
    9:"How diseases spread?",
    10:"What kind of diseases exist?",
    11:"What is cholera?",
    12:"Is COVID-19 the most dangerous virus?",
    13:"How vaccines overcome the virus?"
}

bert_question_embeddings = {}
for i in range(len(questions)) :
    marked_question = clean_text(questions[i][:-1])
    marked_question = ' '.join(marked_question)+"[SEP]"

    tokenized_question = bert_tokenizer.tokenize(marked_question)
    indexed_question = bert_tokenizer.convert_tokens_to_ids(tokenized_question)
    segments_ids_question = [0] * len(tokenized_question)
    question_tokens_tensor = torch.tensor([indexed_question]).to(device)
    question_segments_tensors = torch.tensor([segments_ids_question]).to(device)
    with torch.no_grad():
        outputs = bert_model(question_tokens_tensor, question_segments_tensors)
    hidden_states = outputs[2]
    token_vecs = hidden_states[-2][0]
    bert_question_embeddings[i] = torch.mean(token_vecs, dim=0)

"""For each paper, 20 most similar sentences are chosen and we take the mean score out them. The paper with highest score is the chosen for the answer."""

def find_min(scores) :
    min = 1.0
    index = 0
    for i, score in enumerate(scores):
        if (score < min):
            index = i
  
    return index

def insert_score(best_so_far, new, indeces, index) :
    i_replace = best_so_far.index(min(best_so_far))
    if (best_so_far[i_replace] < new):
        best_so_far[i_replace] = new
        indeces[i_replace] = index
    return best_so_far, indeces

def get_mean(scores) :
    mean = 0
    for score in scores :
        mean += score
    return mean/len(scores)

from scipy.spatial.distance import cosine

for k in range(len(bert_question_embeddings)):
    indeces = {}
    bert_max_score = 0.0
    bert_index = -1
    for i in range(len(bert_doc_embeddings)) :
        if (len(bert_doc_embeddings[i]) == 0) :
            continue
        score = 0.0
        best_sentences = [0.0]*len(bert_doc_embeddings[i])
        indeces[i] = [-1]*len(bert_doc_embeddings[i])
        for j in range(len(bert_doc_embeddings[i])) :
            score = 1 - cosine(bert_doc_embeddings[i][j].cpu(), bert_question_embeddings[k].cpu())
            best_sentences, indeces[i] = insert_score(best_sentences, score, indeces[i], j)
        # print(best_sentences)
        mean = get_mean(best_sentences)
        if (mean > bert_max_score) :
            bert_max_score = mean
            bert_index = i
    print("----------------------------------------------")
    print(f"For question number {k}:")
    print(f"\t{questions[k]}")
    print('With Bert-Model | mean vector similarity: %.4f\n' % bert_max_score)
    print("\nThe answer is in:")
    print(f"\t{files[bert_index]['metadata']['title']}\n")