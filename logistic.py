# -*- coding: utf-8 -*-
"""Logistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i45v6iTUQt_7jjqUnhzptGKsmpRLWT68
"""

from sklearn.model_selection import train_test_split, KFold, cross_validate
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords

import numpy as np
import pandas as pd

import re

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


"""
def expand_contractions(text, contraction_mapping=contraction_map):
    
    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), 
                                      flags=re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = contraction_mapping.get(match)\
                                if contraction_mapping.get(match)\
                                else contraction_mapping.get(match.lower())                       
        expanded_contraction = first_char+expanded_contraction[1:]
        return expanded_contraction
        
    expanded_text = contractions_pattern.sub(expand_match, text)
    expanded_text = re.sub("'", "", expanded_text)
    return expanded_text
"""


def clean_text(text) :
    #CountVectorizer does it
    #text = text.lower() 

    #HTML tags
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE) 
    #Numbers
    text = re.sub(r'\d+', '', text)
    #@-mentions
    text = re.sub(r'(?:@[\w_]+)', '', text)
    #Hashtags
    text = re.sub(r'(?:\#+[\w_]+[\w\'_\-]*[\w_]+)', '', text)
    #Contractions
    #text = expand_contractions(text)
    #Special characters
    text = re.sub(r'[_"\-;%()|+&=*%.,!?:#@$\[\]/]', '', text)
    
    return text

def main() :
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    link = 'https://drive.google.com/file/d/id=1dTIWNpjlrnTQBIQtaGOh0jCRYZiAQO79'
    fluff, id = link.split('=')
    downloaded = drive.CreateFile({'id':id}) 
    downloaded.GetContentFile('SentimentTweets.csv')
    data = pd.read_csv("SentimentTweets.csv",delimiter=',')

    C = data['target'].values
    Docs = data['text'].values
    
    Docs_train, Docs_test, C_train, C_test = train_test_split(Docs, C, test_size = 2/10, random_state = 0 )

    print('Ready to begin preprocessing')
    for i in range(len(Docs)) :
        Docs[i] = clean_text(str(Docs[i]))
    print ('Finished preprocessing\n')

    kf = KFold(n_splits=5, shuffle=True)

    cv = CountVectorizer(max_features=10000)
    
    print('Results with kFold split:\n')
    for train_i, test_i in kf.split(Docs_train) :
        x_train, x_test = Docs[train_i], Docs[test_i]
        y_train, y_test = C[train_i], C[test_i]
        x_train = cv.fit_transform(x_train)
        x_test = cv.transform(x_test)
    
        clf = LogisticRegression(random_state=0, max_iter=600).fit(x_train, y_train)
        y_pred = clf.predict(x_test)
        print(classification_report(y_pred,y_test))

    print('Results with larger dataset (80-20 train-test split:\n')
    x_train = cv.fit_transform(Docs_train)
    x_test = cv.transform(Docs_test)
    clf = LogisticRegression(random_state=0, max_iter=600).fit(x_train, C_train)
    y_pred = clf.predict(x_test)
    print(classification_report(y_pred, C_test))

if __name__ == "__main__" : 
    main()