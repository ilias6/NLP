# -*- coding: utf-8 -*-
"""InferSent-Glove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ErPkf074lifLtUEkaErRv_Bpnc0h38jF

ILIAS KONTONIS 1115201700055
---

Most of the procedure is the same as in SBert-Large.ipynb, so some text cells are skipped.
"""

import tarfile

#Download from original source file cord-19_2020-03-13.tar.gz
"""
import requests

url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz'
r = requests.get(url, allow_redirects=True)
open('cord-19_2020-03-13.tar.gz', 'wb').write(r.content)

tf = tarfile.open('cord-19_2020-03-13.tar.gz')
tf.extractall()
"""
#Download from drive file comm_use_subset.tar.gz

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/file/d/id=1LKQXQ6sUSSQL3ymX0Arp8jk0v3CNNK4U'
fluff, id = link.split('=')
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('comm_use_subset.tar.gz')

tf = tarfile.open(name='comm_use_subset.tar.gz', mode='r:gz')
tf.extractall()

"""Download glove.840.300d.txt for our embeddings"""

import zipfile

link = 'https://drive.google.com/file/d/id=13PLAjN_O6AF6j3vgXNrIg3hc42dmRCa9'
fluff, id = link.split('=')
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('glove.840B.300d.zip')

with zipfile.ZipFile('glove.840B.300d.zip', 'r') as zip_ref:
    zip_ref.extractall()

"""Create the embeddings' array"""

import numpy as np

glove = "./glove.840B.300d.txt"
dim = 300

def loadGloveModel():
    word_embeddings = {}
    f = open(glove, encoding='utf-8')
    for line in f:
        values = line.split()
        word = values[0]
        try :
            coefs = np.asarray(values[1:], dtype='float32')
            word_embeddings[word] = coefs
        except ValueError:
            continue
    f.close()
    return word_embeddings

word_embeddings = loadGloveModel()

!pip install wget

import wget

infersent = wget.download("https://dl.fbaipublicfiles.com/infersent/infersent1.pkl")

"""The following cell has the code of InferSent model of Facebook."""

# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

"""
This file contains the definition of encoders used in https://arxiv.org/pdf/1705.02364.pdf
"""

import numpy as np
import time

import torch
import torch.nn as nn


class InferSent(nn.Module):

    def __init__(self, config):
        super(InferSent, self).__init__()
        self.bsize = config['bsize']
        self.word_emb_dim = config['word_emb_dim']
        self.enc_lstm_dim = config['enc_lstm_dim']
        self.pool_type = config['pool_type']
        self.dpout_model = config['dpout_model']
        self.version = 1 if 'version' not in config else config['version']

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True, dropout=self.dpout_model)

        assert self.version in [1, 2]
        if self.version == 1:
            self.bos = '<s>'
            self.eos = '</s>'
            self.max_pad = True
            self.moses_tok = False
        elif self.version == 2:
            self.bos = '<p>'
            self.eos = '</p>'
            self.max_pad = False
            self.moses_tok = True

    def is_cuda(self):
        # either all weights are on cpu or they are on gpu
        return self.enc_lstm.bias_hh_l0.data.is_cuda

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (bsize)
        # sent: (seqlen x bsize x worddim)
        sent, sent_len = sent_tuple

        # Sort by length (keep idx)
        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent_len_sorted = sent_len_sorted.copy()
        idx_unsort = np.argsort(idx_sort)

        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \
            else torch.from_numpy(idx_sort)
        sent = sent.index_select(1, idx_sort)

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)
        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]

        # Un-sort by length
        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \
            else torch.from_numpy(idx_unsort)
        sent_output = sent_output.index_select(1, idx_unsort)

        # Pooling
        if self.pool_type == "mean":
            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()
            emb = torch.sum(sent_output, 0).squeeze(0)
            emb = emb / sent_len.expand_as(emb)
        elif self.pool_type == "max":
            if not self.max_pad:
                sent_output[sent_output == 0] = -1e9
            emb = torch.max(sent_output, 0)[0]
            if emb.ndimension() == 3:
                emb = emb.squeeze(0)
                assert emb.ndimension() == 2

        return emb

    def set_w2v_path(self, w2v_path):
        self.w2v_path = w2v_path

    def get_word_dict(self, sentences, tokenize=True):
        # create vocab of words
        word_dict = {}
        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]
        for sent in sentences:
            for word in sent:
                if word not in word_dict:
                    word_dict[word] = ''
        word_dict[self.bos] = ''
        word_dict[self.eos] = ''
        return word_dict

    def get_w2v(self, word_dict):
        assert hasattr(self, 'w2v_path'), 'w2v path not set'
        # create word_vec with w2v vectors
        word_vec = {}
        with open(self.w2v_path, encoding='utf-8') as f:
            for line in f:
                word, vec = line.split(' ', 1)
                if word in word_dict:
                    word_vec[word] = np.fromstring(vec, sep=' ')
        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))
        return word_vec

    def get_w2v_k(self, K):
        assert hasattr(self, 'w2v_path'), 'w2v path not set'
        # create word_vec with k first w2v vectors
        k = 0
        word_vec = {}
        with open(self.w2v_path, encoding='utf-8') as f:
            for line in f:
                word, vec = line.split(' ', 1)
                if k <= K:
                    word_vec[word] = np.fromstring(vec, sep=' ')
                    k += 1
                if k > K:
                    if word in [self.bos, self.eos]:
                        word_vec[word] = np.fromstring(vec, sep=' ')

                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):
                    break
        return word_vec

    def build_vocab(self, sentences, tokenize=True):
        assert hasattr(self, 'w2v_path'), 'w2v path not set'
        word_dict = self.get_word_dict(sentences, tokenize)
        self.word_vec = self.get_w2v(word_dict)
        print('Vocab size : %s' % (len(self.word_vec)))

    # build w2v vocab with k most frequent words
    def build_vocab_k_words(self, K):
        assert hasattr(self, 'w2v_path'), 'w2v path not set'
        self.word_vec = self.get_w2v_k(K)
        print('Vocab size : %s' % (K))

    def update_vocab(self, sentences, tokenize=True):
        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'
        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'
        word_dict = self.get_word_dict(sentences, tokenize)

        # keep only new words
        for word in self.word_vec:
            if word in word_dict:
                del word_dict[word]

        # udpate vocabulary
        if word_dict:
            new_word_vec = self.get_w2v(word_dict)
            self.word_vec.update(new_word_vec)
            print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))
        else:
            new_word_vec = []

    def get_batch(self, batch):
        # sent in batch in decreasing order of lengths
        # batch: (bsize, max_len, word_dim)
        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))

        for i in range(len(batch)):
            for j in range(len(batch[i])):
                embed[j, i, :] = self.word_vec[batch[i][j]]

        return torch.FloatTensor(embed)

    def tokenize(self, s):
        from nltk.tokenize import word_tokenize
        if self.moses_tok:
            s = ' '.join(word_tokenize(s))
            s = s.replace(" n't ", "n 't ")  # HACK to get ~MOSES tokenization
            return s.split()
        else:
            return word_tokenize(s)

    def prepare_samples(self, sentences, bsize, tokenize, verbose):
        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else
                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]
        n_w = np.sum([len(x) for x in sentences])

        # filters words without w2v vectors
        for i in range(len(sentences)):
            s_f = [word for word in sentences[i] if word in self.word_vec]
            if not s_f:
                import warnings
                warnings.warn('No words in "%s" (idx=%s) have w2v vectors. \
                               Replacing by "</s>"..' % (sentences[i], i))
                s_f = [self.eos]
            sentences[i] = s_f

        lengths = np.array([len(s) for s in sentences])
        n_wk = np.sum(lengths)
        if verbose:
            print('Nb words kept : %s/%s (%.1f%s)' % (
                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))

        # sort by decreasing length
        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)
        sentences = np.array(sentences)[idx_sort]

        return sentences, lengths, idx_sort

    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):
        tic = time.time()
        sentences, lengths, idx_sort = self.prepare_samples(
                        sentences, bsize, tokenize, verbose)

        embeddings = []
        for stidx in range(0, len(sentences), bsize):
            batch = self.get_batch(sentences[stidx:stidx + bsize])
            if self.is_cuda():
                batch = batch.cuda()
            with torch.no_grad():
                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()
            embeddings.append(batch)
        embeddings = np.vstack(embeddings)

        # unsort
        idx_unsort = np.argsort(idx_sort)
        embeddings = embeddings[idx_unsort]

        if verbose:
            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (
                    len(embeddings)/(time.time()-tic),
                    'gpu' if self.is_cuda() else 'cpu', bsize))
        return embeddings

    def visualize(self, sent, tokenize=True):

        sent = sent.split() if not tokenize else self.tokenize(sent)
        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]

        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):
            import warnings
            warnings.warn('No words in "%s" have w2v vectors. Replacing \
                           by "%s %s"..' % (sent, self.bos, self.eos))
        batch = self.get_batch(sent)

        if self.is_cuda():
            batch = batch.cuda()
        output = self.enc_lstm(batch)[0]
        output, idxs = torch.max(output, 0)
        # output, idxs = output.squeeze(), idxs.squeeze()
        idxs = idxs.data.cpu().numpy()
        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]

        # visualize model
        import matplotlib.pyplot as plt
        x = range(len(sent[0]))
        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]
        plt.xticks(x, sent[0], rotation=45)
        plt.bar(x, y)
        plt.ylabel('%')
        plt.title('Visualisation of words importance')
        plt.show()

        return output, idxs

MODEL_PATH = 'infersent1.pkl'
params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,
                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}
infersent = InferSent(params_model)
infersent.load_state_dict(torch.load(MODEL_PATH))

"""The vocabulary is fixed to 100000 most common words. It is tested that in average, in 1000 articles 15 new words are found, but update_vocab() of InferSent is very time consuming, so new words are ignored and we work with the 1000000 most common."""

W2V_PATH = 'glove.840B.300d.txt'
infersent.set_w2v_path(W2V_PATH)

infersent.build_vocab_k_words(K=100000)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

infersent.to(device)

"""For the similarity defining, norm is chosen instead of the cosine. The smaller the norm (aka distance) the more similar the sentences are. Maybe the cosine similirarity would do fine, but the results show that the most vectors are almost vertical."""

from scipy.spatial.distance import cosine
import nltk
nltk.download('punkt')

q = 'What are the coronoviruses?'
a1 = 'Coronaviruses (CoVs) are common human and animal pathogens that can transmit zoonotically and cause severe respiratory disease syndromes.'
a2 = 'NASCAR stands for National Association for Stock Car Auto Racing.'

q = torch.mean(torch.FloatTensor(infersent.encode(q)), dim=0)
a1 = torch.mean(torch.FloatTensor(infersent.encode(a1)), dim=0)
a2 = torch.mean(torch.FloatTensor(infersent.encode(a2)), dim=0)

print(1 - cosine(a1, q))
print(1 - cosine(a2, q))

print(torch.norm(q - a1, 1).item())
print(torch.norm(q - a2, 1).item())

q = 'What is caused by SARS-COV2?'
a1 = 'Coronavirus disease (COVID-19) is caused by SARS-COV2 and represents the causative agent of a potentially fatal disease that is of great global public health concern.'
a2 = 'In December 2019, a novel coronavirus, called COVID-19, was discovered in Wuhan, China, and has spread to different cities in China as well as to 24 other countries.'
q = torch.mean(torch.FloatTensor(infersent.encode(q)), dim=0)
a1 = torch.mean(torch.FloatTensor(infersent.encode(a1)), dim=0)
a2 = torch.mean(torch.FloatTensor(infersent.encode(a2)), dim=0)

print("Cosine:")
print(1 - cosine(a1, q))
print(1 - cosine(a2, q))
print("Norm:")
print(torch.norm(q - a1, 1))
print(torch.norm(q - a2, 1))

key_words = {"corona", "coronavirus", "coronaviruses","cov","covid","covid-19",
"pandemic","virus","viruses","epidemiology","infection","disease","respiratory",
"sars","sars-cov","sars-cov-2","sars-cov2","pathogens","pathogen","diseases",
"caused","cause","causes","increase","increasement","increases","increased",
"decrease","decreases","decreasement","pandemics","epidemiologist","epidemiologists","discovery",
"discoveries","discovers","discover","spread","spreads","stop","stops", "absorption", 
"stopped","help","helps","helped","helping", "abdominal", "abduction", "abrasion",
"transmit","transmits","effect", "effects", "effective","reported","lyssaviruses","lyssaviruse",
"dna","rna","cells","lungs","symptoms","symptom","vaccine",
"vaccines","therapy","therapies","theory","theoretical","experiment","experiments",
"experimental","system","systems","hiv","cure","cures","yet", "dcir"
"therapeutic", "invading", "invade", "invades", "invasion", "kidney" "organ", "organs"
"ill", "illness", "develop", "developing", "develops", "developed", "necrosis", "analysis",
"genes", "gene", "heart", "unknown", "antibodies", "antibody", "inhibit", "inhibits", "etiology"
"controversial", "infectious", "autoimmune", "nasal", "viral", "pain", "transmission", "m-rna", "mrna",
"epidemic", "death", "deaths", "care", "medical", "attention", "nursing", "oxygene", "abdominal", 
"abduction", "abrasion", "absorption", "accelerator", "acceptance", "accreditation", "acculturation", "acidosis", "activities", "acupuncture", 
"addiction", "adduction", "adenitis", "adipose", "adolescence", "adrenal", "advance", "aerobic", "afebrile", "affection", 
"agar", "agent", "agglutination", "albino", "alginate", "alignment", "alimentary", "alopecia", "alternative", "alveolar", 
"alveoli", "Alzheimer's", "amalgam", "ambulate", "amino", "amputation", "anaerobic", "analgesia", "anaphylactic", "anatomy", 
"anemia", "anesthesia", "anorexia", "anoxia", "antecubital", "anterior", "anterior", "antibody", "anticoagulant", "antigen", 
"antioxidants", "antisepsis", "anuria", "aorta", "aortic", "apathy", "apex", "aphasia", "apical", "apnea", 
"apoplexy", "aquathermia", "aqueous", "aromatherapy", "arrhythmia", "arterial", "arteriole", "arteriosclerosis", "artery", "arthritis", 
"asepsis", "aspirate", "aspiration", "assault", "assistant", "astigmatism", "atherosclerosis", "atrium", "atrophy", "audiologist", 
"audiometer", "auricle", "auscultation", "autoclave", "automated", "autonomic", "autopsy", "avulsion", "axial", "axilla", 
"bachelor's", "bacteria", "bandage", "bandage", "Bartholin's", "basal", "base", "base", "battery", "bed", 
"benign", "bias", "bicuspids", "bifurcated", "bile", "binders", "bioethics", "biohazardous", "biopsy", "bite-wing", 
"bladder", "bland", "blood", "blood", "blood", "bloodborne", "body", "bolus", "bowel", "Bowman's", 
"brachial", "bradycardia", "brain", "bronchi", "bronchioles", "buccal", "buffer", "bulimarexia", "bulimia", "burn", 
"burs", "C", "calcaneus", "calculus", "calorie", "cancer", "cane", "capillary", "carbohydrate-controlled", 
"carbohydrates", "carcinogen", "carcinoma", "cardiac", "cardiac", "cardiopulmonary", "cardiopulmonary", "cardiovascular", "caries", "carious", 
"carpal", "carpule", "catalyst", "cataract", "catheter", "caudal", "cavitation", "cavity", "cell", "cell", 
"cellulose", "cement", "cementum", "central", "centrifuge", "centrosome", "cerebellum", "cerebrospinal", "cerebrovascular", "cerebrum", 
"certification", "cervical", "cervix", "chain", "character", "charge", "chemical", "chemical", "chemotherapy", "Cheyne-Stokes", 
"chiropractic", "cholelithiasis", "cholesterol", "choroid", "chromatin", "chronic", "cilia", "circumduction", "clavicle", "clean", 
"clear-liquid", "client", "clinic", "closed", "coccyx", "cochlea", "colon", "colostomy", "communicable", "compensation", 
"competent", "complementary", "complete", "composite", "compress", "computer-assisted", "computerized", "concave", "confidential", "congenital", 
"conjunctiva", "connective", "constipation", "constrict", "consultation", "contagious", "contamination", "contra", "contract", "contracture", 
"contusion", "conventional-speed", "convex", "convulsion", "cornea", "cortex", "cost", "Cowper's", "cranial", "cranium", 
"cross", "cross-match", "crown", "crust", "crutches", "cryotherapy", "culture", "culture", "cuspid", "custom", 
"cyanosis", "cystitis", "cystoscope", "cytoplasm", "dangling", "day", "deciduous", "decubitus", 
"deduction", "defamation", "defecation", "defense", "defibrillate", "dehydration", "delirium", "delusion", "dementia", "denial", 
"dental", "dental", "dental", "dentin", "dentist", "dentition", "denture", "depression", "dermis", "development", 
"diabetes", "diabetic", "diagnosis", "dialysis", "diaphoresis", "diaphysis", "diarrhea", "diastole", "diastolic", "diathermy", 
"diencephalon", "dietitian", "differential", "digestion", "digital", "dilate", "direct", "disability", "discretion", "disease", 
"disinfection", "dislocation", "disorientation", "displacement", "distal", "distal", "diuretics", "doctorate", "dorsal", "dorsal", 
"dressing", "dry", "dry", "duodenum", "dyspepsia", "dysphagia", "dyspnea", "dysrhythmia", "dystrophy", "dysuria", "early", "early", "echocardiography", "edema", "electrocardiogram", "electroencephalogram", "emblem", "embolus", 
"embryo", "emesis", "emotional", "empathy", "enamel", "endocardium", "endocrine", "endodontics", "endogenous", "endometrium", 
"endoplasmic", "endorsement", "endoscope", "endosteum", "enema", "enunciate", "enuresis", "enzyme", "epidemic", "epidemiology", 
"epidermis", "epididymis", "epigastric", "epiglottis", "epilepsy", "epiphysis", "epistaxis", "epithelial", "ergonomics", "erythema", 
"erythrocyte", "erythrocyte", "erythorcyte", "esophagus", "essential", "ethics", "ethnicity", "ethnocentric", "etiology", "eupnea", 
"eustachian", "eversion", "exacerbation", "excretion", "exocrine", "exogenous", "expectorate", "expiration", "extension", "external", "facial", "facsimile", "fainting", "Fallopian", "fanfold", "fascia", "fasting", "fat", 
"fat-restricted", "fax", "febrile", "feces", "Federation", "femur", "fertilization", "fetus", "fever", "fibula", 
"field", "filing", "firewall", "first", "flatus", "flexion", "fomite", "fontanel", "foramina", "Fowler's", 
"fracture", "frontal", "frostbite", "full", "fungi", "G", "gait", "gait", "gallbladder", 
"gastric", "gastrostomy", "generic", "genes", "genital", "genome", "geriatrics,", "glaucoma", "glomerulus", "glucose", 
"glycosuria", "Golgi", "goniometer", "Gram's", "graphic", "groin", "growth", "gynecology", "H", 
"halitosis", "hantavirus", "hard", "hard", "hardware", "Health", "heart", "heat", "heat", "heat", 
"helminths", "hematemesis", "hematocrit", "hematology", "hematoma", "hematopoiesis", "hematuria", "hemiplegia", "hemodialysis", "hemoglobin", 
"hemolysis", "hemoptysis", "hemorrhage", "hemorrhoids", "hemostat", "heparin", "hepatitis", "high-fiber", "high-protein", "high-velocity", 
"HIPAA", "histology", "holistic", "home", "homeostasis", "horizontal", "hormone", "HOSA", "hospice", "hospital", 
"humerus", "hydrocollator", "hygiene", "hyperglycemia", "hyperopia", "hyperpnea", "hypertension", "hyperthermia", "hypoglycemia", "hypotension", 
"hypothalamus", "hypothermia", "hypothermia", "hypoxia", "I", "ice", "idiopathic", "ileostomy", "ileum", 
"immunity", "impaction", "impression", "incisal", "incision", "incisors", "incontinent", "index", "infancy", "infarction", 
"infection", "inferior", "inflammation", "informed", "ingestion", "inguinal", "inhalation", "input", "inquiry", "insertion", 
"inspiration", "insulin", "insulin", "insurance", "intake", "integrative", "integumentary", "interactive", "intercostal", "interproximal", 
"intestine", "intradermal", "intramuscular", "intravenous", "intubate", "invasion", "invasive", "inversion", "involuntary", "iris", 
"ischemia", "isolation", "jackknife", "jaundice", "jejunum", "joint", 
"kcal-controlled", "ketone", "ketonuria", "kidney", "kilocalorie", "kilojoule", "knee-chest", "labia", 
"labia", "labia", "laboratory", "laceration", "lacrimal", "lactation", "lacteal", "lancet", "laryngeal", "larynx", 
"late", "late", "lateral", "lead", "ledger", "left", "legal", "legal", "lens", "lethargy", 
"leukocyte", "leukocyte", "liability", "libel", "licensure", "life", "ligament", "light", "line", "liner", 
"lingual", "lithotomy", "liver", "living", "low-cholesterol", "low-protein", "low-residue", "low-speed", "lung", "lymph", 
"lymph", "lymphatic", "lymphatic", "lysosomes", "macule", "magnetic", "malignant", "malnutrition", 
"malpractice", "mammogram", "managed", "mandible", "master's", "mastication", "Material", "matriarchal", "maxilla", "meatus", 
"mechanical", "medial", "Medicaid", "medical", "medical", "Medicare", "medication", "Medigap", "medulla", "medulla", 
"medullary", "meiosis", "melanin", "memorandum", "meninges", "menopause", "mental", "mesial", "metabolism", "metacarpal", 
"metastasis", "metatarsal", "microbiology", "microorganism", "microscope", "micturate", "midbrain", "middle", "midsagittal", "midstream", 
"minerals", "mitered", "mitochondria", "mitosis", "mitral", "model", "moist", "moist", "molars", "mouth", 
"mucus", "muscle", "muscle", "myocardial", "myocardium", "myopia", "nasal", "nasal", 
"nasogastric", "nausea", "necrosis", "need", "needle", "negligence", "neonate", "neoplasm", "nephritis", "nephron", 
"nerve", "nerve", "network", "neurology", "neuron", "nocturia", "noninvasive", "nonpathogen", "nonverbal", "nose", 
"nosocomial", "nucleolus", "nucleus", "nutrition", "nutritional", "obese", "objective", "observation", 
"obstetrics", "occlusal", "occult", "occult", "occupational", "occupied", "odontology", "olfactory", "oliguria", "ombudsman", 
"Omnibus", "oncology", "open", "ophthalmologist", "ophthalmology", "ophthalmoscope", "opportunistic", "optician", "optometrist", "oral", 
"oral", "oral", "oral-evacuation", "organ", "organ", "organelles", "origin", "orthodontics", "orthopedics", "orthopnea", 
"orthotist", "os", "ossicles", "osteopathy", "osteoporosis", "ostomy", "otoscope", "output", "ovary", "palate", "palliative", "pallor", "palpation", "pancreas", "pandemic", "panoramic", "Papanicolaou", "papule", 
"paraffin", "paralysis", "paraplegia", "parasite", "parasympathetic", "parathyroid", "parenteral", "paresis", "partial", "patella", 
"pathogen", "pathology", "pathophysiology", "patience", "patients'", "patriarchal", "pediatrics", "pedodontics", "pegboard", "pelvic", 
"percussion", "percussion", "periapical", "pericardium", "perineum", "periodontal", "periodontics", "periodontium", "periosteum", "peripheral", 
"peristalsis", "peritoneal", "permanent", "personal", "personal", "perspiration", "pH", "phalanges", "pharmacology", "pharynx", 
"phlebitis", "phlebotomist", "physiatrist", "physical", "physical", "Physicians'", "physiological", "physiology", "pineal", "pinna", 
"pituitary", "placenta", "plane", "plaque", "plasma", "platelet", "pleura", "podiatrist", "point", "poisoning", 
"polycythemia", "polydipsia", "polyphagia", "polyuria", "pons", "positron", "posterior", "posterior", "postmortem", "postoperative", 
"postpartum", "Power", "prefix", "prejudice", "prenatal", "preoperative", "pressure", "primary", "privileged", "proctoscope", 
"prognosis", "projection", "pronation", "prone", "prophylactic", "prophylaxis", "prostate", "prosthesis", "prosthodontics", "protective", 
"protein", "protoplasm", "protozoa", "proximal", "pruritus", "psychiatry", "psychology", "psychosomatic", "puberty", "pulmonary", 
"pulmonary", "pulp", "pulse", "pulse", "pulse", "pulse", "puncture", "pupil", "pustule", "pyrexia", 
"pyuria", "race", "radial", "radiograph", "radiology", "radiolucent", "radiopaque", "radius", 
"rale", "random", "range", "rate", "rationalization", "read", "reagent", "reality", "recall", "receipt", 
"rectal,", "rectal", "red", "red", "reference", "refractometer", "registration", "rehabilitation", "remission", "repression", 
"resident", "resistant", "respiration", "restoration", "restraints", "retina", "retractor", "reverse", "rheostat", "rhythm", 
"ribs", "rickettsiae", "root", "rotation", "rubber", "S", "safety", "saliva", "salivary", 
"sarcoma", "satisfaction", "scalpel", "scapula", "sclera", "screen", "scrotum", "sebaceous", "secretion", "seizure", 
"self-actualization", "self-esteem", "self-motivation", "semicircular", "seminal", "senile", "senility", "sensitive", "sensitivity", "sepsis", 
"septum", "serrated", "sharps", "shock", "sigmoidoscope", "sign", "signature", "Sims'", "sinus", "sitz", 
"skeleton", "skill", "skin", "slander", "small", "smear", "Snellen", "social", "sodium", "sodium-restricted", 
"soft", "soft", "software", "specific", "speculum", "sphygmomanometer", "spinal", "spinal", "spleen", "splinter", 
"sprain", "sputum", "standard", "statement-receipt", "statistical", "stereotyping", "sterile", "sterile", "sterilization", "sternum", 
"stethoscope", "stoma", "stomach", "stool", "strain", "stress", "stroke", "subcutaneous", "subcutaneous", "subjective", 
"sublingual", "succedaneous", "sudoriferous", "suffix", "suicide", "superior", "supination", "supine", "suppository", "suppression", 
"surgery", "surgical", "surgical", "surgical", "suture", "suture-removal", "sympathetic", "symptom", "syncope", "system", 
"systemic", "systole", "systolic", "tachycardia", "tachypnea", "tactful", "tarsal", "tartar", 
"teamwork", "technician", "technologist", "teeth", "temperature", "temporal", "temporary", "tendon", "tension", "terminal", 
"testes", "thalamus", "therapeutic", "therapy", "thermometer", "thermotherapy", "thoracic", "thoracic", "thrombus", "thymus", 
"thyroid", "tibia", "time", "tissue", "tissue", "tongue", "tonometer", "tonsil", "tort", "tourniquet", 
"towel", "trachea", "tracheostomy", "transdermal", "transfer", "transfusion", "transverse", "Trendelenburg", "triage", "tricuspid", 
"tri-flow", "trifurcated", "tuning", "24-hour", "tympanic", "typing", "ulcer", "ulna", "ulnar", "ultrasonic", 
"ultrasonography", "ultra-speed", "umbilicu", "Universal","National", "uremia", "ureter", "ureterostomy", "urethra", "urinalysis", "urinary", 
"urinar", "urinate", "urine", "urinometer", "urology", "urticaria", "uterus", "vaccine", "varicose", "vascular", 
"vasodilation", "vector", "vein", "venipuncture", "venou", "ventilation", "ventral", "ventricle", "venule", "vertebrae", 
"vertigo", "vesicle", "vestibule", "veterinary", "villi", "virus", "visceral", "vitamins", "void", "volume", 
}

import nltk
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import re

stop_words = set(stopwords.words('english'))

def remove_sentence(sentences):
    if (sentences[-1] == '.') :
        return [sentences[-1]]
    return remove_sentence(sentences[:-1])

def parse_sentence(sentences) :
    temp = []
    while (sentences) :
        word = sentences.pop()
        if (word == '.') :
            return sentences
        if (word in key_words) :
            return sentences+[word]+temp
        temp = [word]+temp
    return []


def clean_text(text) :
    #Lowercase
    text = text.lower()
    
    #Remove numbers
    text = re.sub("\d+", "", text)
    # Seperate punctuation
    text = re.findall(r"[\w']+|[.,!?;]", text)

    new = []
    sentence = []
    for i, word in enumerate(text) :
        sentence.append(word)
        if (word == '.' or word == '!') :
            if (len(sentence) < 5) :
                sentence = []
                continue
            sentence = parse_sentence(sentence)
            sentence = ' '.join(sentence)
            new.append(sentence)
            sentence = []
        elif (word == '?') :
            sentence = []
    return new

wanted_sections = {"conclusion", "conclusions", "Conclusion", "Conclusions",
                   "CONCLUSION", "CONCLUSIONS", "summary", "Summary", "SUMMARY" 
                   "Inference", "inference", "Inferences", "inferences", "INFERENCE",
                   "results", "Results", "RESULTS", "Discussion", "DISCUSSION",
                   "outcome", "OUTCOME", "Outcome"}

from nltk.tokenize import sent_tokenize

"""Unfortunetly, we cannot encode the whole dataset due to insufficient memory. So we keep the half dataset."""

import json
import os
import warnings
warnings.filterwarnings("ignore")

def parse_document(document) :
    info = {}
    info['embs'] = {}
    info['sentences'] = {}
    counter = 0
    temp = []
    for i in range(len(document['body_text'])) :
        if (document['body_text'][i]['section'] not in wanted_sections) :
            continue
        sentences = sent_tokenize(document['body_text'][i]['text'])
        if (temp == sentences) :
            continue
        temp = sentences
        if (sentences) :
            for sentence in sentences :
                if len(sentence) > 15:
                    if (counter and sentence == info['sentences'][counter-1]) :
                        continue
                    info['embs'][counter] = torch.mean(torch.FloatTensor(infersent.encode(sentence)), dim=0)
                    info['sentences'][counter] = sentence
                    # print(sentence)
                    counter += 1

    return info

documents = {}
files = {}
doc_number = 0
labels = []

for file in os.listdir('comm_use_subset'):
    with open("comm_use_subset/"+file, "r") as document:
        if (not doc_number % 1000) :
            print(doc_number)
        document = json.load(document)
        files[doc_number] = document
        documents[doc_number] = parse_document(document)

    doc_number += 1

import pickle

embs = open("glove.pkl", "wb")
pickle.dump(documents, embs)
embs.close()

questions = {
    0:"What are the coronoviruses?",
    1:"What was discovered in Wuhuan in December 2019?",
    2:"What is Coronovirus Disease 2019?",
    3:"What is COVID-19?",
    4:"What is caused by SARS-COV2?",
    5:"How is COVID-19 spread?",
    6:"Where was COVID-19 discovered?",
    7:"How does coronavirus spread?",
    8:"What is SARS-COV2?",
    9:"How diseases spread?",
    10:"What kind of diseases exist?",
    11:"What is cholera?",
    12:"Is COVID-19 the most dangerous virus?",
    13:"How vaccines overcome the virus?"
}

questions_embeddings = {}
for i, question in enumerate(questions) :
    sentence = questions[i]
    questions_embeddings[i] = torch.mean(torch.FloatTensor(infersent.encode(sentence)), dim=0)

def max_index(scores) :
    max = float('inf')
    index = -1
    for i, score in enumerate(scores):
        if (score < max):
            index = i
    return index

def calc_score(d, l) :
    return (200*d - l)/201

def compare(s1, s2) :
    score1 = calc_score(s1)
    score2 = calc_score(s2)
    if (score1 <= score2) :
        return 0
    else :
        return 1  

def insert_score(best_so_far, new, indeces, index) :

    i_replace = best_so_far.index(max(best_so_far))
    if (best_so_far[i_replace] > new):
        best_so_far[i_replace] = new
        indeces[i_replace] = index
    return best_so_far, indeces

def get_mean(scores) :
    mean = 0
    for score in scores :
        mean += score
    return mean/len(scores)

for i in range(len(questions)) :
    indeces = {}
    min_score = float('Inf')
    doc_index = -1
    for j in range(len(documents)) :
        score = 0.0
        best_sentences = [float('Inf')]*10
        indeces[j] = [-1]*10
        for k in range(len(documents[j]['sentences'])) :
            doc_embedding = documents[j]['embs'][k]
            score = torch.norm(doc_embedding - questions_embeddings[i], 1).item()
            score = calc_score(score, len(documents[j]['sentences'][k]))
            best_sentences, indeces[j] = insert_score(best_sentences, score, indeces[j], k)
        mean = get_mean(best_sentences)
        if (mean < min_score) :
            min_score = mean
            doc_index = i
    print("----------------------------------------------")
    print(f"For question number {i}:")
    print(f"\t{questions[i]}")
    print('With Inference-Glove | mean vector distance: %.3f\n' % min_score)
    print("\nThe answer is in:")
    print(f"\t{files[doc_index]['metadata']['title']}\n")
    print("----------------------------------------------")