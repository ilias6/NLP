# -*- coding: utf-8 -*-
"""RidgeReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xHjEV1vA-dOZOWFiaxFuQobspRu7z8uX
"""

import matplotlib.pyplot as plt
from mlxtend.preprocessing import standardize
from sklearn.model_selection import train_test_split 
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# function to compute predictions created for the current dataset
# where y = w0 + w1x1 +...+w8x8 
def predict(X, theta):
    pred = np.dot(X, theta[0:8]) + theta[8]
    return pred

# function to compute gradient of error 
def gradient(X, y, theta, pen):
    h = np.dot(X, theta) 
    grad = (2/X.shape[0])*np.dot(X.transpose(), (h - y)) + pen*theta
    return grad 

# function to compute Ridge Reg Loss
def cost(X, y, theta, pen): 
    h = np.dot(X, theta)
    J = (np.dot((h - y).transpose(), (h - y)))/X.shape[0]
    #print (J[0])
    return J[0] 

# function to perform mini-batch gradient descent
def gradientDescent(X, y, steps, pen): 
    X = np.c_[ X, np.ones(X.shape[0]) ] #add a column of ones to X for the bias term
    theta = np.zeros((X.shape[1], 1)) #create inital weights w0, w1..wd
    error_list = []
    for s in range(steps):
        theta = theta - 0.05*gradient(X, y, theta, pen)
        #error_list.append(cost(X, y, theta, pen)) #useful for plotting changes when using different batch sizes

    return theta, error_list

def learningSched(t0, t1, t) :
    return t0/(t+t1)

def stohasticGradientDesc(X, y, n_epochs, pen) :
    X = np.c_[ X, np.ones(X.shape[0]) ] #add a column of ones to X for the bias term
    theta = np.zeros((X.shape[1], 1)) #create inital weights w0, w1..wd
    error_list = []
    for epoch in range(n_epochs) :
        for i in range(X.shape[0]) :
            random_i = np.random.randint(X.shape[0])
            xi = X[random_i:random_i+1]
            yi = y[random_i:random_i+1]
            theta = theta - learningSched(5, 50, epoch*X.shape[0]+i)*gradient(xi, yi, theta, pen)
            #error_list.append(cost(X, y, theta, pen)) #useful for plotting changes when using different batch sizes
    return theta, error_list

def mini_batchGradientDesc(X, y, iterations, pen, batch_size) :
    X = np.c_[ X, np.ones(X.shape[0]) ] #add a column of ones to X for the bias term
    theta = np.zeros((X.shape[1], 1)) #create inital weights w0, w1..wd
    error_list = []

    for i in range(iterations) :
        for j in range(int(X.shape[0]/batch_size)) :
            random_i = np.random.randint(X.shape[0]-batch_size)
            xi = X[random_i:random_i+batch_size]
            yi = y[random_i:random_i+batch_size]
            theta = theta - 0.0001*gradient(xi, yi, theta, pen)
            #error_list.append(cost(X, y, theta, pen)) #useful for plotting changes when using different batch sizes
    return theta, error_list 

def data_prepare(data) :
    x = data.iloc[:, 1:-1].values
    y = data.iloc[:, -1:].values
    return (x, y)

def main() :
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    link = 'https://drive.google.com/open?id=1VUn2WKkKeRXwH02K9bqH98KjPxrUmgXh'
    fluff, id = link.split('=')
    downloaded = drive.CreateFile({'id':id}) 
    downloaded.GetContentFile('HousingData.csv')
    data = pd.read_csv("HousingData.csv",delimiter=',')

    (x,y) = data_prepare(data)
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 1/100, random_state = 0 )

    x_train = standardize(x_train, columns=[0,1,2,3,4,5,6,7])
    x_test = standardize(x_test, columns=[0,1,2,3,4,5,6,7]) 
    y_train = standardize(y_train, columns=[0])
    y_test = standardize(y_test, columns=[0])

    print('Perfoming Batch gradient descent\n')
    (theta, error) = gradientDescent(x_train, y_train, 1000, 0.001)
    y_pred = predict(x_test, theta)
    print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))
    plt.plot(x_test, y_pred, "r-")
    plt.plot(x_test, y_test, "b.")
    plt.show()

    print('Perfoming Stohastic gradient descent\n')
    (theta, error) = stohasticGradientDesc(x_train, y_train, 50, 0.01)
    y_pred = predict(x_test, theta)
    print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))
    plt.plot(x_test, y_pred, "r-")
    plt.plot(x_test, y_test, "b.")
    plt.show()

    print('Perfoming mini-Batch gradient descent\n')
    (theta, error) = mini_batchGradientDesc(x_train, y_train, 1000, 0.01, 50)
    y_pred = predict(x_test, theta)
    print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))
    plt.plot(x_test, y_pred, "r-")
    plt.plot(x_test, y_test, "b.")
    plt.show()

if __name__ == "__main__" : 
    main()